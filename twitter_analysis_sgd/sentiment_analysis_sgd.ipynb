{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Description\n",
    "\n",
    "The \"Sentiment Analysis using SGD Classifier\" project is a comprehensive data analysis and natural language processing (NLP) endeavor aimed at extracting valuable insights from textual data. Sentiment analysis is a branch of NLP that involves determining the emotional tone or sentiment expressed in text, which can range from positive and neutral to negative.\n",
    "\n",
    "Project Objectives:\n",
    "\n",
    "* Sentiment Classification: The primary goal of this project is to develop a robust sentiment analysis system using the Stochastic Gradient Descent (SGD) Classifier. This machine learning algorithm is well-suited for large-scale text classification tasks.\n",
    "\n",
    "* Data Collection and Preparation: The project will involve gathering a diverse dataset of textual data, such as social media posts from twitter. This data will be cleaned, preprocessed, and annotated with sentiment labels.\n",
    "\n",
    "* Feature Engineering: To effectively train the SGD Classifier, the project will explore various text preprocessing techniques, including tokenization, stop-word removal, and vectorization methods like TF-IDF (Term Frequency-Inverse Document Frequency).\n",
    "\n",
    "* Model Training and Evaluation: The SGD Classifier will be trained on the preprocessed dataset to classify text into sentiment categories (e.g., positive, neutral, negative). The project will employ appropriate evaluation metrics such as accuracy to assess the model's performance.\n",
    "\n",
    "* Hyperparameter Tuning: Fine-tuning the SGD Classifier's hyperparameters will be essential to optimize its performance for sentiment analysis. Grid search or randomized search techniques may be employed.\n",
    "\n",
    "* Deployment: Once the model achieves satisfactory results, it can be deployed as a practical sentiment analysis tool. This could involve creating a web application, API, or integration with existing systems to analyze and classify text in real-time.\n",
    "\n",
    "* Future Improvements: As part of continuous improvement, the project may explore advanced NLP models, transfer learning techniques, or domain-specific sentiment analysis to enhance accuracy and applicability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\fortune\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\fortune\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\fortune\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm # instantly makes loops show a smart progress meter â€” just wrap any iterable with tqdm(iterable)\n",
    "from sklearn.feature_extraction.text import CountVectorizer # to convert a collection of text to a matrix of token counts\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer # to convert a collection of text to a matrix of TF-IDF features\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import pipeline\n",
    "from sklearn.kernel_approximation import (RBFSampler, Nystroem)\n",
    "from sklearn import preprocessing\n",
    "import string\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_csv_path = \"kaggle_twitter_analysis\\\\\"\n",
    "input_csv_filename = \"MLUnige2021_train.csv\"\n",
    "test_csv_filename = \"MLUnige2021_test.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>lyx_query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>original_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2063391019</td>\n",
       "      <td>Sun Jun 07 02:28:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BerryGurus</td>\n",
       "      <td>@BreeMe more time to play with you BlackBerry ...</td>\n",
       "      <td>@BreeMe more time to play with you BlackBerry ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000525676</td>\n",
       "      <td>Mon Jun 01 22:18:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>peterlanoie</td>\n",
       "      <td>Failed attempt at booting to a flash drive. Th...</td>\n",
       "      <td>Failed attempt at booting to a flash drive. Th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2218180611</td>\n",
       "      <td>Wed Jun 17 22:01:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>will_tooker</td>\n",
       "      <td>@msproductions Well ain't that the truth. Wher...</td>\n",
       "      <td>@msproductions Well ain't that the truth. Wher...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2190269101</td>\n",
       "      <td>Tue Jun 16 02:14:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammutimer</td>\n",
       "      <td>@Meaghery cheers Craig - that was really sweet...</td>\n",
       "      <td>@Meaghery cheers Craig - that was really sweet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2069249490</td>\n",
       "      <td>Sun Jun 07 15:31:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ohaijustin</td>\n",
       "      <td>I was reading the tweets that got send to me w...</td>\n",
       "      <td>I was reading the tweets that got send to me w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  emotion    tweet_id                          date lyx_query  \\\n",
       "0   0        1  2063391019  Sun Jun 07 02:28:13 PDT 2009  NO_QUERY   \n",
       "1   1        0  2000525676  Mon Jun 01 22:18:53 PDT 2009  NO_QUERY   \n",
       "2   2        0  2218180611  Wed Jun 17 22:01:38 PDT 2009  NO_QUERY   \n",
       "3   3        1  2190269101  Tue Jun 16 02:14:47 PDT 2009  NO_QUERY   \n",
       "4   4        0  2069249490  Sun Jun 07 15:31:58 PDT 2009  NO_QUERY   \n",
       "\n",
       "          user                                               text  \\\n",
       "0   BerryGurus  @BreeMe more time to play with you BlackBerry ...   \n",
       "1  peterlanoie  Failed attempt at booting to a flash drive. Th...   \n",
       "2  will_tooker  @msproductions Well ain't that the truth. Wher...   \n",
       "3   sammutimer  @Meaghery cheers Craig - that was really sweet...   \n",
       "4   ohaijustin  I was reading the tweets that got send to me w...   \n",
       "\n",
       "                                       original_text  \n",
       "0  @BreeMe more time to play with you BlackBerry ...  \n",
       "1  Failed attempt at booting to a flash drive. Th...  \n",
       "2  @msproductions Well ain't that the truth. Wher...  \n",
       "3  @Meaghery cheers Craig - that was really sweet...  \n",
       "4  I was reading the tweets that got send to me w...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_df = pd.read_csv(input_csv_path + input_csv_filename)\n",
    "test_df = pd.read_csv(input_csv_path + test_csv_filename)\n",
    "twitter_df[\"original_text\"] = twitter_df[\"text\"]\n",
    "test_df[\"original_text\"] = test_df[\"text\"]\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading stop wordsin english, stemmer and lemmatizer.\n",
    "stop_words = set(json.load(open(\"stop_words_english.json\", encoding=\"utf-8\")))\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Stop Words Removal: Stop words removal is the process of eliminating common and insignificant words like \"the\" and \"is\" from text data to improve processing efficiency and focus on meaningful content.\n",
    "\n",
    "* Stemming: Stemming is the technique of reducing words to their base or root form, such as converting \"running\" to \"run,\" for normalization and improved retrieval of similar words.\n",
    "\n",
    "* Lemmatization: Lemmatization is the process of reducing words to their dictionary or lemma form, considering the context and semantics, to preserve meaning and ensure interpretability in NLP tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>emotion</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>date</th>\n",
       "      <th>lyx_query</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>original_text</th>\n",
       "      <th>wc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2063391019</td>\n",
       "      <td>Sun Jun 07 02:28:13 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>BerryGurus</td>\n",
       "      <td>time play BlackBerry</td>\n",
       "      <td>@BreeMe more time to play with you BlackBerry ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2000525676</td>\n",
       "      <td>Mon Jun 01 22:18:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>peterlanoie</td>\n",
       "      <td>Failed attempt booting flash drive Then failed...</td>\n",
       "      <td>Failed attempt at booting to a flash drive. Th...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2218180611</td>\n",
       "      <td>Wed Jun 17 22:01:38 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>will_tooker</td>\n",
       "      <td>Well  ' truth Where ' damn autolock disable Co...</td>\n",
       "      <td>@msproductions Well ain't that the truth. Wher...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2190269101</td>\n",
       "      <td>Tue Jun 16 02:14:47 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>sammutimer</td>\n",
       "      <td>cheers Craig sweet reply ' pumped</td>\n",
       "      <td>@Meaghery cheers Craig - that was really sweet...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2069249490</td>\n",
       "      <td>Sun Jun 07 15:31:58 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ohaijustin</td>\n",
       "      <td>reading tweets send lying phone face dropped  ...</td>\n",
       "      <td>I was reading the tweets that got send to me w...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  emotion    tweet_id                          date lyx_query  \\\n",
       "0   0        1  2063391019  Sun Jun 07 02:28:13 PDT 2009  NO_QUERY   \n",
       "1   1        0  2000525676  Mon Jun 01 22:18:53 PDT 2009  NO_QUERY   \n",
       "2   2        0  2218180611  Wed Jun 17 22:01:38 PDT 2009  NO_QUERY   \n",
       "3   3        1  2190269101  Tue Jun 16 02:14:47 PDT 2009  NO_QUERY   \n",
       "4   4        0  2069249490  Sun Jun 07 15:31:58 PDT 2009  NO_QUERY   \n",
       "\n",
       "          user                                               text  \\\n",
       "0   BerryGurus                               time play BlackBerry   \n",
       "1  peterlanoie  Failed attempt booting flash drive Then failed...   \n",
       "2  will_tooker  Well  ' truth Where ' damn autolock disable Co...   \n",
       "3   sammutimer                  cheers Craig sweet reply ' pumped   \n",
       "4   ohaijustin  reading tweets send lying phone face dropped  ...   \n",
       "\n",
       "                                       original_text  wc  \n",
       "0  @BreeMe more time to play with you BlackBerry ...   3  \n",
       "1  Failed attempt at booting to a flash drive. Th...  13  \n",
       "2  @msproductions Well ain't that the truth. Wher...  12  \n",
       "3  @Meaghery cheers Craig - that was really sweet...   6  \n",
       "4  I was reading the tweets that got send to me w...   8  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dictionary to convert negations to simpler terms.\n",
    "negations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",\n",
    "                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n",
    "                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n",
    "                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n",
    "                \"mustn't\":\"must not\"}\n",
    "neg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')\n",
    "\n",
    "# Function to remove stop words, perform stemming and lemmatization after converting the texts to tokens\n",
    "def preprocess(strs):\n",
    "    lower_case = strs.lower()\n",
    "    neg_handled = neg_pattern.sub(lambda x: negations_dic[x.group()], lower_case)\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", neg_handled)\n",
    "    strs = re.sub(r'@[\\w]+','',strs)\n",
    "    strs = re.sub(r'<.*?>', '', strs)\n",
    "    text = re.sub(r'[^a-zA-z.,!?/:;\\\"\\'\\s]', '', strs)\n",
    "    text = word_tokenize(text)\n",
    "    text = (\" \".join([word for word in text if word not in stop_words and word not in string.punctuation and len(word)>1]))\n",
    "    text = re.sub(r'\\b\\w{1,3}\\b', '', text)\n",
    "    return text\n",
    "\n",
    "# Function to count the number of words in the tweets.\n",
    "def word_counter(text):\n",
    "    return len(text.split())\n",
    "\n",
    "twitter_df[\"text\"] = twitter_df[\"text\"].apply(lambda x: preprocess(x))\n",
    "test_df[\"text\"] = test_df[\"text\"].apply(lambda x: preprocess(x))\n",
    "twitter_df[\"wc\"] = twitter_df[\"text\"].apply(lambda x: word_counter(x))\n",
    "test_df[\"wc\"] = test_df[\"text\"].apply(lambda x: word_counter(x))\n",
    "twitter_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test (80-20 split)\n",
    "train_df, test_df = train_test_split(twitter_df, test_size=0.2, stratify=twitter_df[\"emotion\"])\n",
    "x_train = train_df[\"text\"]\n",
    "x_test = test_df[\"text\"]\n",
    "y_train = train_df[\"emotion\"]\n",
    "y_true = test_df[\"emotion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfvec = TfidfVectorizer(stop_words = \"english\",\n",
    "                                  analyzer = 'word',\n",
    "                                  lowercase = False,\n",
    "                                  use_idf = False,\n",
    "                                  ngram_range = (1,2))\n",
    "        \n",
    "X_train = tfidfvec.fit_transform(x_train)\n",
    "\n",
    "X_test = tfidfvec.transform(x_test)                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Term Frequency-Inverse Document Frequency (TF-IDF) vectorizer is a popular technique used in natural language processing (NLP) and information retrieval to convert a collection of text documents into numerical feature vectors. It is often used as a preprocessing step before applying machine learning algorithms. Here's how TF-IDF vectorization works:\n",
    "\n",
    "* Term Frequency (TF):\n",
    "\n",
    "Term Frequency (TF) measures the frequency of a term (word) within a specific document. It is calculated as the number of times a term appears in a document divided by the total number of terms in that document.\n",
    "\n",
    "TF is useful because it helps capture the importance of terms within individual documents. Terms that appear frequently in a document are often more important in representing the content of that document.\n",
    "\n",
    "The formula for TF is: TF(t, d) = (Number of times term t appears in document d) / (Total number of terms in document d)\n",
    "\n",
    "* Inverse Document Frequency (IDF):\n",
    "\n",
    "Inverse Document Frequency (IDF) measures the importance of a term across a collection of documents (corpus). It is calculated as the logarithm of the total number of documents in the corpus divided by the number of documents that contain the term, with 1 added to the denominator to prevent division by zero.\n",
    "\n",
    "IDF is used to identify terms that are relatively rare and, therefore, potentially more significant in distinguishing documents. Terms that appear in many documents have lower IDF scores, while terms that appear in few documents have higher IDF scores.\n",
    "\n",
    "The formula for IDF is: IDF(t, D) = log((Total number of documents in the corpus D) / (Number of documents containing term t in corpus D)) + 1\n",
    "\n",
    "* TF-IDF Weighting:\n",
    "\n",
    "The TF-IDF weight of a term in a document combines the TF and IDF values to represent the term's importance both in the document and across the entire corpus. It is calculated as the product of TF and IDF.\n",
    "\n",
    "The TF-IDF weight emphasizes terms that are frequent within a document (high TF) and relatively rare in the corpus (high IDF).\n",
    "\n",
    "The formula for TF-IDF is: TF-IDF(t, d, D) = TF(t, d) * IDF(t, D)\n",
    "\n",
    "* Vectorization:\n",
    "\n",
    "Once TF-IDF scores are calculated for each term in each document, they form a matrix where rows represent documents, columns represent terms, and each cell contains the TF-IDF score of a term in a document.\n",
    "\n",
    "This TF-IDF matrix is often used as input for machine learning algorithms, where each document is represented as a vector of TF-IDF scores, and terms are the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training, evaluation and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier(alpha=2.1727548793319498e-06, eta0=0.01, learning_rate='adaptive',\n",
       "              loss='modified_huber', penalty='elasticnet')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'penalty': ['elasticnet'],\n",
    "    'loss': ['modified_huber'],\n",
    "    'learning_rate': ['adaptive'],\n",
    "    'eta0': [0.01],\n",
    "    'alpha': [0.0000021727548793319498],  # You can specify multiple values to tune here\n",
    "}\n",
    "\n",
    "\n",
    "clf = SGDClassifier()\n",
    "grid_search = GridSearchCV(clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_params = grid_search.best_params_\n",
    "best_classifier = grid_search.best_estimator_\n",
    "best_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.74016796875"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_classifier.score(X_test, y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* The Stochastic Gradient Descent (SGD) Classifier is an efficient and versatile machine learning algorithm well-suited for large-scale classification tasks, capable of handling high-dimensional datasets and online learning scenarios.\n",
    "* The core of the SGD Classifier is the Stochastic Gradient Descent optimization algorithm. Unlike traditional Gradient Descent, which computes the gradient of the cost function using the entire dataset, SGD calculates the gradient based on a single randomly selected data point at each iteration. This makes it highly efficient for large datasets.\n",
    "* It incorporates L1 and L2 regularization, which helps prevent overfitting and improves the model's generalization ability. Regularization terms are controlled by hyperparameters like alpha.\n",
    "* The eta0 parameter represents the initial learning rate, which can be adapted during training using learning rate schedules like \"constant,\" \"optimal,\" or \"adaptive.\" Adaptive learning rates often prove beneficial as they automatically adjust based on the progress of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
